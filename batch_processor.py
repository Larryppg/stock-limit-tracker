"""
æ‰¹é‡å¤„ç†æ¨¡å— - MVPç‰ˆæœ¬ï¼ˆè¿‘3ä¸ªæœˆæ•°æ®ï¼‰
"""
import pandas as pd
import sqlite3
from datetime import datetime
import database
import data_fetcher
import limit_calculator
import config


def run_mvp_pipeline(recent_days: int = None):
    """
    è¿è¡ŒMVPæµç¨‹ï¼šè·å–æœ€è¿‘Nå¤©æ•°æ®å¹¶è®¡ç®—è¿æ¿é«˜åº¦
    
    å‚æ•°:
        recent_days: æœ€è¿‘å¤©æ•°ï¼Œé»˜è®¤ä½¿ç”¨configä¸­çš„é…ç½®
    """
    if recent_days is None:
        recent_days = config.DEFAULT_RECENT_DAYS
    
    print("="*60)
    print(f"å¼€å§‹è¿è¡ŒMVPæµç¨‹ - æœ€è¿‘ {recent_days} å¤©æ•°æ®")
    print("="*60)
    # #region agent log
    try:
        from data_fetcher import _dbg_log as _dbg_log_local
        import importlib
        import database as _db_mod
        _db_mod = importlib.reload(_db_mod)
        _dbg_log_local("H4", "batch_processor.py:run_mvp_pipeline", "enter", {
            "recent_days": int(recent_days),
            "database_file": getattr(_db_mod, "__file__", ""),
            "db_module_version": getattr(_db_mod, "DB_MODULE_VERSION", "missing"),
            "db_has_version": hasattr(_db_mod, "DB_MODULE_VERSION")
        })
    except Exception:
        pass
    # #endregion agent log
    
    # æ­¥éª¤1: åˆå§‹åŒ–æ•°æ®åº“
    print("\n[1/5] åˆå§‹åŒ–æ•°æ®åº“...")
    database.init_database()
    
    # æ­¥éª¤2: è·å–è‚¡ç¥¨åˆ—è¡¨
    print("\n[2/5] è·å–è‚¡ç¥¨åˆ—è¡¨...")
    stocks = data_fetcher.get_stock_list()
    
    if stocks.empty:
        print("âœ— è·å–è‚¡ç¥¨åˆ—è¡¨å¤±è´¥ï¼Œæµç¨‹ç»ˆæ­¢")
        return
    
    # ä¿å­˜è‚¡ç¥¨å…ƒä¿¡æ¯
    database.save_stock_meta(stocks)
    print(f"âœ“ è‚¡ç¥¨å…ƒä¿¡æ¯å·²ä¿å­˜ï¼Œå…± {len(stocks)} åª")
    
    # æ­¥éª¤3: è·å–å¸‚åœºæ•°æ®
    print(f"\n[3/5] è·å–æœ€è¿‘ {recent_days} å¤©å¸‚åœºæ•°æ®...")
    start_date, end_date = data_fetcher.get_recent_trading_days(recent_days)
    print(f"æ—¥æœŸèŒƒå›´: {start_date} ~ {end_date}")
    # #region agent log
    try:
        _dbg_log_local("H4", "batch_processor.py:run_mvp_pipeline", "date_range", {"start_date": start_date, "end_date": end_date})
    except Exception:
        pass
    # #endregion agent log
    
    # MVP æ¨¡å¼å¯é€‰é™åˆ¶è‚¡ç¥¨æ•°é‡ï¼ˆé»˜è®¤ä¸é™åˆ¶ï¼‰
    mvp_limit = getattr(config, "MVP_LIMIT_STOCKS", 0)
    if mvp_limit and int(mvp_limit) > 0:
        print(f"\nâš ï¸  MVPæ¨¡å¼ï¼šåªå¤„ç†å‰ {int(mvp_limit)} åªè‚¡ç¥¨ç”¨äºå¿«é€ŸéªŒè¯")
        stock_codes = stocks['code'].head(int(mvp_limit)).tolist()
    else:
        stock_codes = stocks['code'].tolist()
    
    # #region agent log
    try:
        _dbg_log_local("H9", "batch_processor.py:run_mvp_pipeline", "before_fetch_market", {
            "codes_count": len(stock_codes),
            "mvp_limit": int(mvp_limit) if mvp_limit else 0
        })
    except Exception:
        pass
    # #endregion agent log

    try:
        # ä¼˜å…ˆä½¿ç”¨ Tushareï¼ˆMVPï¼‰ï¼Œä¸å†å›é€€ AkShare
        if getattr(config, "TUSHARE_USE_IN_MVP", False):
            max_calls = int(getattr(config, "TUSHARE_MAX_CALLS_PER_MIN", 50))
            # #region agent log
            try:
                _dbg_log_local("H18", "batch_processor.py:run_mvp_pipeline", "mvp_use_tushare_only", {
                    "max_calls": int(max_calls)
                })
            except Exception:
                pass
            # #endregion agent log

            market_data = pd.DataFrame()
            # æ˜¯å¦ä½¿ç”¨æŒ‰æ—¥æœŸæ‰¹é‡æ‹‰å–æ¨¡å¼ï¼ˆéœ€è¦è¾ƒé«˜Tushareç§¯åˆ†ï¼‰
            use_by_date_mode = getattr(config, "TUSHARE_USE_BY_DATE_MODE", False)
            if not mvp_limit and use_by_date_mode:
                # === æŒ‰æ—¥æœŸæ‰¹é‡æ‹‰å–æ¨¡å¼ï¼ˆéœ€è¦è¾ƒé«˜ç§¯åˆ†ï¼Œå…è´¹ç”¨æˆ·æ•°æ®ä¸å…¨ï¼‰ ===
                print("\nâš™ï¸  ä½¿ç”¨æŒ‰æ—¥æœŸæ‰¹é‡æ‹‰å–æ¨¡å¼...")
                market_data = data_fetcher.fetch_market_data_tushare_by_date(
                    start_date,
                    end_date
                )
                if market_data.empty:
                    # #region agent log
                    try:
                        _dbg_log_local("H29", "batch_processor.py:run_mvp_pipeline", "by_date_empty_fallback", {
                            "start_date": start_date,
                            "end_date": end_date
                        })
                    except Exception:
                        pass
                    # #endregion agent log
                else:
                    # è¿‡æ»¤åˆ°å½“å‰è‚¡ç¥¨åˆ—è¡¨ï¼ˆé¿å…æ„å¤–å“ç§ï¼‰
                    if "code" in market_data.columns:
                        market_data = market_data[market_data["code"].isin(set(stock_codes))]
                    # #region agent log
                    try:
                        _dbg_log_local("H28", "batch_processor.py:run_mvp_pipeline", "market_data_filtered", {
                            "rows": int(len(market_data)),
                            "unique_codes": int(market_data["code"].nunique()) if "code" in market_data.columns else 0
                        })
                    except Exception:
                        pass
                    # #endregion agent log

                    # #region agent log
                    try:
                        _dbg_log_local("H9", "batch_processor.py:run_mvp_pipeline", "after_fetch_market", {"rows": int(len(market_data))})
                    except Exception:
                        pass
                    # #endregion agent log

                # ä¿å­˜å¸‚åœºæ•°æ®ï¼ˆç›´æ¥å†™å…¥ï¼Œé¿å…æ—§æ¨¡å—ç¼“å­˜ï¼‰
                print("\nä¿å­˜å¸‚åœºæ•°æ®åˆ°æ•°æ®åº“...")
                try:
                    chunk_size = int(getattr(config, "DAILY_SAVE_CHUNK_SIZE", 50000))
                    # #region agent log
                    try:
                        _dbg_log_local("H35", "batch_processor.py:run_mvp_pipeline", "save_daily_direct_start", {
                            "rows": int(len(market_data)),
                            "chunk_size": int(chunk_size)
                        })
                    except Exception:
                        pass
                    # #endregion agent log
                    conn = sqlite3.connect(config.DB_PATH, timeout=30)
                    conn.execute("PRAGMA busy_timeout=30000;")
                    conn.execute("PRAGMA journal_mode=WAL;")
                    market_data.to_sql(
                        'daily_market_data',
                        conn,
                        if_exists='append',
                        index=False,
                        chunksize=chunk_size,
                        method='multi'
                    )
                    conn.commit()
                    conn.close()
                    # #region agent log
                    try:
                        _dbg_log_local("H35", "batch_processor.py:run_mvp_pipeline", "save_daily_direct_done", {
                            "rows": int(len(market_data))
                        })
                    except Exception:
                        pass
                    # #endregion agent log
                    print(f"âœ“ å¸‚åœºæ•°æ®å·²ä¿å­˜ï¼Œå…± {len(market_data)} æ¡è®°å½•")
                except Exception as e:
                    print(f"âš ï¸  ä¿å­˜æ•°æ®æ—¶å‡ºç°è­¦å‘Š: {e}")
                    print("ç»§ç»­æ‰§è¡Œåç»­æ­¥éª¤...")

                    # æ­¥éª¤4: è®¡ç®—è¿æ¿é«˜åº¦
                    print("\n[4/5] è®¡ç®—è¿æ¿é«˜åº¦...")
                    # åªä½¿ç”¨å·²è·å–æ•°æ®å¯¹åº”çš„è‚¡ç¥¨å…ƒä¿¡æ¯
                    stocks_subset = stocks[stocks['code'].isin(stock_codes)]
                    limit_results = limit_calculator.calculate_batch_chain(market_data, stocks_subset)
                    batch_mode = False

            # åˆ¤æ–­æ˜¯å¦ä½¿ç”¨æ‰¹é‡é€è‚¡æ‹‰å–æ¨¡å¼
            # æ¡ä»¶ï¼š1) æ•°æ®ä¸ºç©ºï¼ˆæŒ‰æ—¥æœŸæ¨¡å¼å¤±è´¥æˆ–æœªå¯ç”¨ï¼‰ 2) æœªé™åˆ¶è‚¡ç¥¨æ•° 3) è‚¡ç¥¨æ•°è¶…è¿‡å•æ¬¡è°ƒç”¨ä¸Šé™
            use_batch = market_data.empty and (not mvp_limit) and (len(stock_codes) > max_calls)
            if not use_by_date_mode and not mvp_limit:
                # æœªä½¿ç”¨æŒ‰æ—¥æœŸæ¨¡å¼æ—¶ï¼Œå¼ºåˆ¶ä½¿ç”¨é€è‚¡æ‰¹é‡æ‹‰å–
                use_batch = True
                print("\nâš™ï¸  ä½¿ç”¨é€è‚¡æ‰¹é‡æ‹‰å–æ¨¡å¼ï¼ˆç§¯åˆ†è¦æ±‚ä½ï¼Œä½†è°ƒç”¨æ¬¡æ•°å¤šï¼‰...")
            if use_batch:
                batch_size = int(getattr(config, "MVP_BATCH_SIZE", config.FETCH_BATCH_SIZE))
                total_batches = (len(stock_codes) + batch_size - 1) // batch_size
                print(f"\nâš™ï¸  MVPæ‰¹é‡æ¨¡å¼ï¼š{total_batches} æ‰¹ï¼Œæ¯æ‰¹ {batch_size} åªè‚¡ç¥¨")
                limit_results_list = []

                for batch_idx in range(total_batches):
                    start_idx = batch_idx * batch_size
                    end_idx = min((batch_idx + 1) * batch_size, len(stock_codes))
                    batch_codes = stock_codes[start_idx:end_idx]
                    # #region agent log
                    try:
                        _dbg_log_local("H26", "batch_processor.py:run_mvp_pipeline", "batch_start", {
                            "batch_idx": int(batch_idx + 1),
                            "codes_count": int(len(batch_codes)),
                            "batch_size": int(batch_size),
                        })
                    except Exception:
                        pass
                    # #endregion agent log

                    batch_data, rate_limited, call_count = data_fetcher.fetch_market_data_tushare(
                        batch_codes,
                        start_date,
                        end_date,
                        max_calls=max_calls,
                        rate_limit_enable=True,
                        run_label="mvp_batch"
                    )
                    if rate_limited:
                        print(f"âš ï¸  MVP æ‰¹æ¬¡ {batch_idx + 1} è§¦å‘ Tushare é™é€Ÿï¼ˆå·²è°ƒç”¨ {call_count} æ¬¡ï¼‰ï¼Œåœæ­¢åç»­æ‰¹æ¬¡")
                        break
                    if batch_data.empty:
                        print(f"âš ï¸  MVP æ‰¹æ¬¡ {batch_idx + 1} è¿”å›ç©ºæ•°æ®ï¼Œè·³è¿‡")
                        continue

                    # ä¿å­˜å¸‚åœºæ•°æ®
                    print("\nä¿å­˜å¸‚åœºæ•°æ®åˆ°æ•°æ®åº“...")
                    try:
                        database.save_daily_data(batch_data)
                        print(f"âœ“ æ‰¹æ¬¡ {batch_idx + 1} å¸‚åœºæ•°æ®å·²ä¿å­˜ï¼Œå…± {len(batch_data)} æ¡è®°å½•")
                    except Exception as e:
                        print(f"âš ï¸  æ‰¹æ¬¡ {batch_idx + 1} ä¿å­˜æ•°æ®è­¦å‘Š: {e}")

                    # è®¡ç®—è¿æ¿é«˜åº¦
                    print("\n[4/5] è®¡ç®—è¿æ¿é«˜åº¦...")
                    batch_stocks = stocks[stocks['code'].isin(batch_codes)]
                    batch_results = limit_calculator.calculate_batch_chain(batch_data, batch_stocks)
                    if not batch_results.empty:
                        try:
                            database.save_limit_results(batch_results)
                            print(f"âœ“ æ‰¹æ¬¡ {batch_idx + 1} è¿æ¿åˆ†æç»“æœå·²ä¿å­˜ï¼Œå…± {len(batch_results)} æ¡è®°å½•")
                        except Exception as e:
                            print(f"âš ï¸  æ‰¹æ¬¡ {batch_idx + 1} ä¿å­˜ç»“æœè­¦å‘Š: {e}")
                        limit_results_list.append(batch_results)

                if not limit_results_list:
                    print("âœ— MVPæ‰¹é‡æ¨¡å¼æœªè·å–åˆ°ä»»ä½•æœ‰æ•ˆè¿æ¿ç»“æœï¼Œæµç¨‹ç»ˆæ­¢")
                    return
                limit_results = pd.concat(limit_results_list, ignore_index=True)
                batch_mode = True
            else:
                if market_data.empty:
                    market_data, rate_limited, call_count = data_fetcher.fetch_market_data_tushare(
                        stock_codes,
                        start_date,
                        end_date,
                        max_calls=max_calls,
                        rate_limit_enable=True,
                        run_label="mvp"
                    )
                    if rate_limited:
                        print(f"âš ï¸  MVP è§¦å‘ Tushare é™é€Ÿï¼ˆå·²è°ƒç”¨ {call_count} æ¬¡ï¼‰ï¼Œå¯é‡æ–°è¿è¡Œç»§ç»­è·å–")
                    if market_data.empty:
                        print("âœ— Tushare è¿”å›ç©ºæ•°æ®ï¼ŒMVPç»ˆæ­¢ï¼ˆæŒ‰äº¤æ˜“æ—¥æ‹‰å–å¤±è´¥ï¼Œä¸”é€è‚¡æ‹‰å–ä¸ºç©ºï¼‰")
                        return

                    # #region agent log
                    try:
                        _dbg_log_local("H9", "batch_processor.py:run_mvp_pipeline", "after_fetch_market", {"rows": int(len(market_data))})
                    except Exception:
                        pass
                    # #endregion agent log

                    # ä¿å­˜å¸‚åœºæ•°æ®
                    print("\nä¿å­˜å¸‚åœºæ•°æ®åˆ°æ•°æ®åº“...")
                    try:
                        database.save_daily_data(market_data)
                        print(f"âœ“ å¸‚åœºæ•°æ®å·²ä¿å­˜ï¼Œå…± {len(market_data)} æ¡è®°å½•")
                    except Exception as e:
                        print(f"âš ï¸  ä¿å­˜æ•°æ®æ—¶å‡ºç°è­¦å‘Š: {e}")
                        print("ç»§ç»­æ‰§è¡Œåç»­æ­¥éª¤...")

                    # æ­¥éª¤4: è®¡ç®—è¿æ¿é«˜åº¦
                    print("\n[4/5] è®¡ç®—è¿æ¿é«˜åº¦...")
                    # åªä½¿ç”¨å·²è·å–æ•°æ®å¯¹åº”çš„è‚¡ç¥¨å…ƒä¿¡æ¯
                    stocks_subset = stocks[stocks['code'].isin(stock_codes)]
                    limit_results = limit_calculator.calculate_batch_chain(market_data, stocks_subset)
                    batch_mode = False
        else:
            print("âœ— æœªå¯ç”¨ Tushareï¼ˆMVPï¼‰ï¼Œä¸”å·²ç¦ç”¨AkShareå›é€€")
            return
    except Exception as e:
        # #region agent log
        try:
            _dbg_log_local("H9", "batch_processor.py:run_mvp_pipeline", "fetch_market_exception", {"error": str(e), "error_type": type(e).__name__})
        except Exception:
            pass
        # #endregion agent log
        raise
    
    if limit_results.empty:
        print("âœ— è¿æ¿è®¡ç®—å¤±è´¥ï¼Œæµç¨‹ç»ˆæ­¢")
        return
    
    # ä¿å­˜è¿æ¿åˆ†æç»“æœï¼ˆéæ‰¹é‡æ¨¡å¼ï¼‰
    if not batch_mode:
        print("\nä¿å­˜è¿æ¿åˆ†æç»“æœåˆ°æ•°æ®åº“...")
        try:
            database.save_limit_results(limit_results)
            print(f"âœ“ è¿æ¿åˆ†æç»“æœå·²ä¿å­˜ï¼Œå…± {len(limit_results)} æ¡è®°å½•")
        except Exception as e:
            print(f"âš ï¸  ä¿å­˜ç»“æœæ—¶å‡ºç°è­¦å‘Š: {e}")
    
    # æ­¥éª¤5: ç”Ÿæˆæ‘˜è¦ç»Ÿè®¡
    print("\n[5/5] ç”Ÿæˆæ•°æ®æ‘˜è¦...")
    generate_summary(limit_results, end_date)
    
    print("\n" + "="*60)
    print("âœ“ MVPæµç¨‹å®Œæˆï¼")
    print("="*60)


def run_full_backfill(start_date: str = None, end_date: str = None):
    """
    å…¨é‡å†å²æ•°æ®å›å¡«
    
    å‚æ•°:
        start_date: å¼€å§‹æ—¥æœŸ YYYYMMDDï¼Œé»˜è®¤ä½¿ç”¨configé…ç½®
        end_date: ç»“æŸæ—¥æœŸ YYYYMMDDï¼Œé»˜è®¤ä¸ºä»Šå¤©
    """
    if start_date is None:
        start_date = config.HISTORY_START_DATE
    
    if end_date is None:
        end_date = datetime.now().strftime('%Y%m%d')
    
    print("="*60)
    print(f"å¼€å§‹å…¨é‡å†å²å›å¡«: {start_date} ~ {end_date}")
    print("="*60)
    
    # åˆå§‹åŒ–æ•°æ®åº“
    print("\n[1/4] åˆå§‹åŒ–æ•°æ®åº“...")
    database.init_database()
    
    # è·å–è‚¡ç¥¨åˆ—è¡¨
    print("\n[2/4] è·å–è‚¡ç¥¨åˆ—è¡¨...")
    stocks = data_fetcher.get_stock_list()
    
    if stocks.empty:
        print("âœ— è·å–è‚¡ç¥¨åˆ—è¡¨å¤±è´¥")
        return
    
    database.save_stock_meta(stocks)
    print(f"âœ“ è‚¡ç¥¨å…ƒä¿¡æ¯å·²ä¿å­˜ï¼Œå…± {len(stocks)} åª")
    
    # æ‰¹é‡è·å–å†å²æ•°æ®
    print(f"\n[3/4] æ‰¹é‡è·å–å†å²æ•°æ®...")
    print(f"âš ï¸  å…¨é‡å›å¡«å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´ï¼ˆé¢„è®¡2-4å°æ—¶ï¼‰")
    
    stock_codes = stocks['code'].tolist()
    
    # åˆ†æ‰¹å¤„ç†ï¼Œé¿å…å†…å­˜æº¢å‡º
    batch_size = config.FETCH_BATCH_SIZE
    total_batches = (len(stock_codes) + batch_size - 1) // batch_size
    
    # #region agent log
    try:
        from data_fetcher import _dbg_log as _dbg_log_local
        _dbg_log_local("H19", "batch_processor.py:run_full_backfill", "enter", {
            "start_date": start_date,
            "end_date": end_date,
            "batch_size": int(batch_size),
            "total_batches": int(total_batches)
        })
    except Exception:
        pass
    # #endregion agent log

    for batch_idx in range(total_batches):
        start_idx = batch_idx * batch_size
        end_idx = min((batch_idx + 1) * batch_size, len(stock_codes))
        batch_codes = stock_codes[start_idx:end_idx]
        
        print(f"\nå¤„ç†æ‰¹æ¬¡ {batch_idx + 1}/{total_batches}...")
        
        # è·å–æ‰¹æ¬¡æ•°æ®ï¼ˆä»…ä½¿ç”¨ Tushareï¼‰
        if not getattr(config, "TUSHARE_USE_IN_BACKFILL", False):
            print("âœ— æœªå¯ç”¨ Tushare å›å¡«ï¼Œä¸”å·²ç¦ç”¨å…¶ä»–æ•°æ®æº")
            return
        # #region agent log
        try:
            _dbg_log_local("H19", "batch_processor.py:run_full_backfill", "backfill_use_tushare_only", {
                "batch_idx": int(batch_idx + 1),
                "codes_count": int(len(batch_codes)),
                "max_calls": int(getattr(config, "BACKFILL_RATE_LIMIT_CALLS_PER_MIN", 45))
            })
        except Exception:
            pass
        # #endregion agent log
        batch_data, rate_limited, call_count = data_fetcher.fetch_market_data_tushare(
            batch_codes,
            start_date,
            end_date,
            max_calls=getattr(config, "BACKFILL_RATE_LIMIT_CALLS_PER_MIN", 45),
            rate_limit_enable=getattr(config, "BACKFILL_RATE_LIMIT_ENABLE", True),
            run_label="backfill"
        )
        if rate_limited:
            print(f"âš ï¸  å›å¡«è§¦å‘ Tushare é™é€Ÿï¼ˆå·²è°ƒç”¨ {call_count} æ¬¡ï¼‰ï¼Œåœæ­¢åç»­æ‰¹æ¬¡")
            break
        
        if not batch_data.empty:
            # ä¿å­˜å¸‚åœºæ•°æ®
            try:
                database.save_daily_data(batch_data)
            except Exception as e:
                print(f"âš ï¸  æ‰¹æ¬¡ {batch_idx + 1} ä¿å­˜æ•°æ®è­¦å‘Š: {e}")
            
            # è®¡ç®—è¿æ¿
            batch_stocks = stocks[stocks['code'].isin(batch_codes)]
            batch_results = limit_calculator.calculate_batch_chain(batch_data, batch_stocks)
            
            if not batch_results.empty:
                try:
                    database.save_limit_results(batch_results)
                except Exception as e:
                    print(f"âš ï¸  æ‰¹æ¬¡ {batch_idx + 1} ä¿å­˜ç»“æœè­¦å‘Š: {e}")
    
    print("\n[4/4] å›å¡«å®Œæˆï¼")
    print("="*60)


def run_daily_update(target_date: str = None):
    """
    æ¯æ—¥å¢é‡æ›´æ–°
    
    å‚æ•°:
        target_date: ç›®æ ‡æ—¥æœŸ YYYYMMDDï¼Œé»˜è®¤ä¸ºä»Šå¤©
    """
    if target_date is None:
        target_date = datetime.now().strftime('%Y%m%d')
    
    print("="*60)
    print(f"å¼€å§‹æ¯æ—¥å¢é‡æ›´æ–°: {target_date}")
    print("="*60)
    
    # è·å–è‚¡ç¥¨åˆ—è¡¨
    print("\n[1/3] è·å–è‚¡ç¥¨åˆ—è¡¨...")
    stocks = data_fetcher.get_stock_list()
    
    if stocks.empty:
        print("âœ— è·å–è‚¡ç¥¨åˆ—è¡¨å¤±è´¥")
        return
    
    # æ›´æ–°è‚¡ç¥¨å…ƒä¿¡æ¯
    database.save_stock_meta(stocks)
    
    # è·å–å½“æ—¥æ•°æ®
    print(f"\n[2/3] è·å– {target_date} æ•°æ®...")
    stock_codes = stocks['code'].tolist()
    
    daily_data = data_fetcher.fetch_market_data(stock_codes, target_date, target_date)
    
    if daily_data.empty:
        print(f"âœ— {target_date} æ— äº¤æ˜“æ•°æ®ï¼ˆå¯èƒ½ä¸ºä¼‘å¸‚æ—¥ï¼‰")
        return
    
    # ä¿å­˜å½“æ—¥æ•°æ®
    try:
        database.save_daily_data(daily_data)
        print(f"âœ“ {target_date} å¸‚åœºæ•°æ®å·²ä¿å­˜")
    except Exception as e:
        print(f"âš ï¸  ä¿å­˜æ•°æ®è­¦å‘Š: {e}")
    
    # è®¡ç®—è¿æ¿ï¼ˆéœ€è¦ç»“åˆå†å²æ•°æ®ï¼‰
    print(f"\n[3/3] è®¡ç®— {target_date} è¿æ¿é«˜åº¦...")
    
    # ä¸ºäº†è®¡ç®—è¿æ¿ï¼Œéœ€è¦è·å–æ¯åªè‚¡ç¥¨çš„å‰ä¸€äº¤æ˜“æ—¥çŠ¶æ€
    # ç®€åŒ–ç‰ˆï¼šé‡æ–°è®¡ç®—æœ€è¿‘30å¤©çš„è¿æ¿çŠ¶æ€
    lookback_days = 30
    start_date, _ = data_fetcher.get_recent_trading_days(lookback_days)
    
    all_results = []
    for code in stock_codes[:50]:  # MVP: åªå¤„ç†å‰50åª
        historical_data = database.get_stock_daily_data(code, start_date, target_date)
        
        if not historical_data.empty:
            stock_info = stocks[stocks['code'] == code].iloc[0]
            limit_ratio = stock_info['limit_ratio']
            
            result = limit_calculator.calculate_single_stock_chain(
                historical_data, code, limit_ratio
            )
            
            # åªä¿å­˜ç›®æ ‡æ—¥æœŸçš„ç»“æœ
            result = result[result['date'] == target_date]
            if not result.empty:
                all_results.append(result)
    
    if all_results:
        final_results = pd.concat(all_results, ignore_index=True)
        try:
            database.save_limit_results(final_results)
            print(f"âœ“ {target_date} è¿æ¿åˆ†æç»“æœå·²ä¿å­˜")
        except Exception as e:
            print(f"âš ï¸  ä¿å­˜ç»“æœè­¦å‘Š: {e}")
    
    print("\n" + "="*60)
    print("âœ“ æ¯æ—¥æ›´æ–°å®Œæˆï¼")
    print("="*60)


def generate_summary(limit_results: pd.DataFrame, date: str = None):
    """ç”Ÿæˆæ•°æ®æ‘˜è¦ç»Ÿè®¡"""
    if date is None:
        # ä½¿ç”¨æœ€æ–°æ—¥æœŸ
        date = limit_results['date'].max()
    
    print(f"\nğŸ“Š æ•°æ®æ‘˜è¦ ({date}):")
    print("-" * 40)
    
    # å½“æ—¥æ¶¨åœç»Ÿè®¡
    daily_data = limit_results[limit_results['date'] == date]
    
    if not daily_data.empty:
        limit_count = daily_data['limit_status'].sum()
        fried_count = daily_data['is_fried'].sum()
        yizi_count = len(daily_data[daily_data['board_type'] == 'yizi'])
        
        print(f"æ¶¨åœæ•°é‡: {limit_count}")
        print(f"ç‚¸æ¿æ•°é‡: {fried_count}")
        print(f"ä¸€å­—æ¿æ•°é‡: {yizi_count}")
        
        # è¿æ¿é«˜åº¦åˆ†å¸ƒ
        print("\nè¿æ¿é«˜åº¦åˆ†å¸ƒ:")
        for height in range(1, 11):
            count = len(daily_data[daily_data['chain_height'] == height])
            if count > 0:
                print(f"  {height}æ¿: {count}åª")
        
        # é«˜è¿æ¿è‚¡ç¥¨
        high_chain = daily_data[daily_data['chain_height'] >= 3].sort_values(
            'chain_height', ascending=False
        )
        
        if not high_chain.empty:
            print(f"\n3æ¿åŠä»¥ä¸Šè‚¡ç¥¨ (å…±{len(high_chain)}åª):")
            for _, row in high_chain.head(10).iterrows():
                print(f"  {row['code']}: {int(row['chain_height'])}æ¿ ({row['board_type']})")
    
    print("-" * 40)


if __name__ == '__main__':
    # è¿è¡ŒMVPæµç¨‹
    run_mvp_pipeline(recent_days=90)
